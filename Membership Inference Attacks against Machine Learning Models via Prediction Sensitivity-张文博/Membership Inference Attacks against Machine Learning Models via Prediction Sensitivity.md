# Abstract
- 机器学习易受到成员推理攻击

- 提出Aster来解决这一问题，只需要机器学习黑盒模型API和待推理的数据样本

- **理论基础**：与非训练数据（即测试数据）相比，完全训练的ML模型的训练数据通常具有较低的预测灵敏度。较低的灵敏度意味着当在相应的特征空间中扰动训练样本的特征值时，从目标模型获得的扰动样本的预测趋于与原始预测一致。

- **方法概述**：用雅可比矩阵量化预测灵敏度，该矩阵可以反映每个特征的扰动与相应预测变化之间的关系。然后，我们将具有较低值的样本视为训练数据。

# Introduction
- ML模型容易受到多种攻击，包括对抗攻击、模型窃取攻击、模型反转攻击和隐私侵犯攻击。

- 本文关注成员推理攻击（MIA），其目标是确定样本是否属于给定模型的训练集。MIA可能会严重危及模型的数据安全。

    > 例如：银行训练并发布了一个预测用户信用级别的ML模型用于贷款，如果能通过黑盒MIA攻击知道某人在不在训练集中，就能知道他是不是这家银行的客户。

- 大多数MIA需要有先验知识，比如知道模型的结构、知道训练集的数据分布等。（然而大多数模型只提供黑盒访问接口，包括谷歌和亚马逊的一些服务）
    > 在此插入上次组会的内容